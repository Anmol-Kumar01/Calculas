{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEuhDVj4DkMgpcnpiBGQS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anmol-Kumar01/Calculas/blob/main/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3z0IfAdrBSbk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent \n",
        "It is a widely used algorithm in machine learning and optimization.\n",
        "It minimizes a cost function by adjusting model parameters iteratively. \n",
        "It starts with initial parameters, computes gradients, and updates parameters in the opposite direction of gradients scaled by a learning rate. \n",
        "\n",
        "This process repeats until a stopping \n",
        "criterion is met. Gradient descent is crucial for optimizing models and solving complex problems. Variations like batch, stochastic, and mini-batch gradient descent offer trade-offs in efficiency and convergence.\n",
        "\n",
        "functions in multiple varibles are as follows:\n",
        "$$θ = θ - α\\cdot ∇f(θ) $$\n",
        "\n",
        ">$* \\quad θ$ represents the vector of parameters of model.\n",
        "\n",
        "> $* \\quad α$ is learning rate which controls the length of the step taken in the direction opposite to the gradient.\n",
        "\n",
        "> $* \\quad f(θ)$ represents the gradient of the cost function $f$ $$w.r.t ... θ$$.\n",
        "\n"
      ],
      "metadata": {
        "id": "qbuYhUS_Dt3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Minima using gradient decent method:\n",
        "$$f(x) = sin(x)$$\n",
        "For this function in one variable gradient descent will updated as follows:\n",
        "$$x_{n+1} = x_{n} - \\alpha ∇f(θ)$$\n",
        "where:\n",
        "\n",
        "\n",
        "> $* \\quad x_{n}$ is starting guessing point on function.\n",
        "\n",
        "> $* \\quad α$ is learning rate which controls the length of the step taken in the direction opposite to the gradient."
      ],
      "metadata": {
        "id": "z80uDdakGGok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autograd.numpy as np\n",
        "from autograd import grad as g"
      ],
      "metadata": {
        "id": "rOyQBlf1F-Ez"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating differentiation of function.\n",
        "def cos_func(x):\n",
        "    '''\n",
        "    Return: sin function.\n",
        "\n",
        "    Parameter x: It is a argument given to function.\n",
        "    Precondition: x is int, float or any angle in radians.\n",
        "    '''\n",
        "    return np.cos(x)\n",
        "gradient = g(cos_func)"
      ],
      "metadata": {
        "id": "sWOY8ZPaHWf8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating minima .\n",
        "x_n = 1.0             # start randomly choosen point.\n",
        "alpha = 1                  # learnig rate.\n",
        "start = True\n",
        "while True:\n",
        "    if gradient(x_n) > 0:\n",
        "        X_N = x_n - alpha * gradient(x_n)\n",
        "        x_n -= alpha\n",
        "    if gradient(x_n) < 0:           # iterative process will stop if gdradient is negative.\n",
        "        break\n",
        "print(x_n/np.pi)         # minima in radian."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiWlFfSxHqBZ",
        "outputId": "bc96c982-1ba3-4d5c-a5e2-74e891e38f23"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3183098861837907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-tFT_lQgJK05"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating by gradient descent:\n",
        "\n",
        " for: $$\\quad f{(x)} = x^2$$"
      ],
      "metadata": {
        "id": "EDCwSjqQJMyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding minima by using gradient descent method\n",
        "def square(x):\n",
        "    '''\n",
        "    Return: x ** 2 function.\n",
        "\n",
        "    Parameter x: It is a argument given to function.\n",
        "    Precondition: x is int, float or any angle in radians.\n",
        "    '''\n",
        "    return x ** 2\n",
        "gradient = g(square)"
      ],
      "metadata": {
        "id": "DxpPI_eHIeBe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating minima.\n",
        "x_n = 100.0             # initially randomly choosen point.\n",
        "alpha = 0.001                  # learning factor.\n",
        "val = True\n",
        "while val:\n",
        "    if gradient(x_n) > 0:\n",
        "        xN = x_n - alpha * gradient(x_n)\n",
        "        x_n -= alpha\n",
        "    if gradient(x_n) < 0:           # This will stop the iterative process if dradient is negative.\n",
        "        break\n",
        "print(xN)         # Giving value of minima."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agq42ex7KABJ",
        "outputId": "35e948b0-170d-420d-f3a9-98fc6ab86b63"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009979998867890678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating minima for the function given below using the gradient descent:\n",
        "$$f(x) = x^2 + y^2$$"
      ],
      "metadata": {
        "id": "4vbRi4WBKTDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining functions.\n",
        "def new_func(X):\n",
        "    '''\n",
        "    Return: x ** 2 + y ** 2 function.\n",
        "\n",
        "    Parameter X: It is a argument given to function.\n",
        "    Precondition: x is int, float or any angle in radians.\n",
        "    '''\n",
        "    x, y = X\n",
        "    return x ** 2 + y ** 2\n",
        "gradient = g(new_func)\n"
      ],
      "metadata": {
        "id": "VWOsonVvJEeB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating minima.\n",
        "x_n = [float(input(f\"enter value {i} start with:\")) for i in range(1, 3)]             # initially randomly choosen point.\n",
        "alpha = 0.001                # learning factor.\n",
        "previous = [0.0, 0.0]   # accumulator to keep previous values track\n",
        "val = True\n",
        "while val:\n",
        "    x_n[0] = x_n[0] - alpha*(gradient(x_n)[0]) # gradient decent for first variable\n",
        "    x_n[1] = x_n[1] - alpha*(gradient(x_n)[1]) # gradient decent for second variable\n",
        "    if abs(gradient(x_n)[0] - gradient(previous)[0])<0.0001 and abs(gradient(x_n)[1] - gradient(previous)[1])<0.0001:\n",
        "        val = False\n",
        "        print(x_n)\n",
        "    previous = x_n.copy()    # avoiding loss of values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP0IJ3y5K2mE",
        "outputId": "ede1c73d-7529-400f-92c1-b0a3cd482768"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter value 1 start with:11.25\n",
            "enter value 2 start with:10.56\n",
            "[0.02492843834726187, 0.023399494128629746]\n"
          ]
        }
      ]
    }
  ]
}
